{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FT(object):\n",
    "    \"\"\" \n",
    "    A Fenwick tree is analogous to a binary heap and provides an \n",
    "    efficient prefix sum for counts associated with particular \n",
    "    element indices \n",
    "    \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def lsb(self, i):\n",
    "        return ((i) & (-i))\n",
    "     \n",
    "    def __init__(self, size):\n",
    "        self.elements = None\n",
    "        self.reset(size)\n",
    "    \n",
    "    def sum(self, i):\n",
    "        result = 0\n",
    "        i += 1\n",
    "        while i > 0:\n",
    "            result += self.elements[i]\n",
    "            i -= FT.lsb(i)\n",
    "        return result\n",
    "    \n",
    "    def inc_by(self, i, ct):\n",
    "        i += 1\n",
    "        while i < len(self.elements) - 1:\n",
    "            self.elements[i] += ct\n",
    "            i += FT.lsb(i)\n",
    "    \n",
    "    def reset(self, size=None):\n",
    "        if self.elements is None \\\n",
    "        or (size is not None and size != self.size()):\n",
    "            self.elements = np.zeros(size + 1)\n",
    "        else:\n",
    "            np.ndarray.fill(self.elements, 0.0)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.elements) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will define a `TDigest` class that implements the `update` operation for inserting data elements into the sketch. This tutorial implementation does not provide the `merge` operation, which is useful for combining partial results in a distributed computing setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDigest(object):\n",
    "    COMPRESSION_DEFAULT = 50.0 / 100.0\n",
    "    INIT_SIZE_DEFAULT = 5\n",
    "    K = 10.0 * 50.0\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.centers = np.zeros(1)\n",
    "        self.masses = np.zeros(1)\n",
    "        self.nclusters = 0\n",
    "        self.mass = 0.0\n",
    "    \n",
    "    def __init__(self, compression=COMPRESSION_DEFAULT, max_discrete=0, size=INIT_SIZE_DEFAULT):\n",
    "        self.compression = compression\n",
    "        self.max_discrete = max_discrete\n",
    "        self.ft = FT(size)\n",
    "        self._reset()\n",
    "    \n",
    "    def new_cluster(self, insertion_point, sample, weight):\n",
    "        # note that this will allocate up to two new arrays for \n",
    "        # every created cluster (don't do this in real life!)\n",
    "        \n",
    "        if self.nclusters >= self.ft.size():\n",
    "            self.ft.reset(self.nclusters)\n",
    "        else:\n",
    "            self.ft.reset()\n",
    "        \n",
    "        self.centers = np.insert(self.centers, insertion_point, sample)\n",
    "        self.masses = np.insert(self.masses, insertion_point, weight)\n",
    "        for (k, mass) in enumerate(self.masses):\n",
    "            self.ft.inc_by(k, mass)\n",
    "\n",
    "        self.nclusters += 1\n",
    "        self.mass += weight\n",
    "    \n",
    "    def _closest(self, sample):\n",
    "        insertion_point = np.searchsorted(self.centers, sample, 'right')\n",
    "        \n",
    "        if insertion_point == self.nclusters:\n",
    "            return insertion_point - 1\n",
    "        \n",
    "        if self.centers[insertion_point] == sample \\\n",
    "        or insertion_point == 0:\n",
    "            # the closest cluster center is around this \n",
    "            # sample or is smaller than any cluster center; \n",
    "            # we can rinsertion_pointeturn it unmodified\n",
    "            return insertion_point\n",
    "        \n",
    "        # the insertion point is between two clusters; choose the closest\n",
    "        dl = sample - self.centers[insertion_point - 1]\n",
    "        dr = self.centers[insertion_point] - sample\n",
    "        return (dl < dr) and (insertion_point - 1) or insertion_point\n",
    "            \n",
    "    \n",
    "    def update(self, sample, weight=1.0, maybeRecluster=True):\n",
    "        def _update(sample, weight):\n",
    "            if self.nclusters == 0:\n",
    "                # this is our first observation\n",
    "                self.centers[0] = sample\n",
    "                self.mass = self.masses[0] = weight\n",
    "                self.ft.inc_by(1, weight)\n",
    "                self.nclusters += 1\n",
    "                return\n",
    "                \n",
    "            if self.nclusters <= self.max_discrete:\n",
    "                # we can just introduce a new cluster without\n",
    "                # expanding beyond the maximum number of discrete\n",
    "                # values we're willing to track\n",
    "                insertion_point = np.searchsorted(self.centers, sample, 'right')\n",
    "                if insertion_point < self.nclusters \\\n",
    "                and self.centers[insertion_point] == sample:\n",
    "                    # landed on existing cluster\n",
    "                    self.mass += weight\n",
    "                    self.masses[insertion_point] += weight\n",
    "                    self.ft.inc_by(insertion_point, weight)\n",
    "                else:\n",
    "                    # we need to create a new cluster but don't have\n",
    "                    # to recluster anything\n",
    "                    self.new_cluster(insertion_point, sample, weight)\n",
    "                return\n",
    "\n",
    "            insertion_point = self._closest(sample)\n",
    "            if insertion_point < self.nclusters \\\n",
    "            and sample == self.centers[insertion_point]:\n",
    "                # we have a cluster centered around this sample\n",
    "                self.mass += weight\n",
    "                self.masses[insertion_point] += weight\n",
    "                self.ft.inc_by(insertion_point, weight)\n",
    "                return\n",
    "            \n",
    "            m = self.masses[insertion_point]\n",
    "            \n",
    "            q = (self.ft.sum(insertion_point - 1) + (m / 2.0)) / self.mass\n",
    "            \n",
    "            ub = self.compression * self.mass * q * (1.0 - q)\n",
    "            \n",
    "            delta_mass = min(weight, max(0.0, ub - m))\n",
    "            \n",
    "            remaining_mass = weight - delta_mass\n",
    "            if delta_mass > 0.0:\n",
    "                dc = delta_mass * (sample - self.centers[insertion_point]) / (m + delta_mass)\n",
    "                self.centers[insertion_point] += dc\n",
    "                self.mass += delta_mass\n",
    "                self.masses[insertion_point] += delta_mass\n",
    "                self.ft.inc_by(insertion_point, delta_mass)\n",
    "            if remaining_mass > 0.0:\n",
    "                insertion_point += (sample < self.centers[insertion_point] and 0 or 1)\n",
    "                self.new_cluster(insertion_point, sample, remaining_mass)\n",
    "        \n",
    "        _update(sample, weight)\n",
    "        \n",
    "        if (maybeRecluster \\\n",
    "        and self.nclusters > self.max_discrete \\\n",
    "        and self.nclusters > self.r()):\n",
    "            self.recluster()\n",
    "    \n",
    "    def recluster(self):\n",
    "        indices = list(range(self.nclusters))\n",
    "        np.random.shuffle(indices)\n",
    "        old_centers = self.centers\n",
    "        old_masses = self.masses\n",
    "        self._reset()\n",
    "        \n",
    "        for i in indices:\n",
    "            self.update(old_centers[i], old_masses[i], False)\n",
    "    \n",
    "    def _rcovj(self, x):\n",
    "        \"\"\" return the left index of a right cover \"\"\"\n",
    "        j = np.searchsorted(self.centers, x, 'right')\n",
    "        if j == self.nclusters:\n",
    "            return j - 1\n",
    "        \n",
    "        if self.centers[j] == x:\n",
    "            return j\n",
    "        \n",
    "        return j - 1\n",
    "    \n",
    "    def _rmcovj(self, m):\n",
    "        assert(self.nclusters >= 2)\n",
    "        assert((m >= 0.0) and (m <= self.mass))\n",
    "        beg = 0\n",
    "        mbeg = 0.0\n",
    "        end = self.nclusters - 1\n",
    "        mend = self.mass\n",
    "        while (end - beg) > 1:\n",
    "            mid = int((beg + end) / 2)\n",
    "            mmid = self.ft.sum(mid)\n",
    "            if m >= mmid:\n",
    "                beg = mid\n",
    "                mbeg = mmid\n",
    "            else:\n",
    "                end = mid\n",
    "                mend = mmid\n",
    "        return beg\n",
    "        \n",
    "    def cdf(self, x):\n",
    "        j1 = self._rcovj(x)\n",
    "        if j1 < 0:\n",
    "            return 0.0\n",
    "        if j1 == self.nclusters - 1:\n",
    "            return 1.0\n",
    "        \n",
    "        j2 = j1 + 1\n",
    "        c1, c2 = self.centers[j1], self.centers[j2]\n",
    "        tm1, tm2 = self.masses[j1], self.masses[j2]\n",
    "        s = self.ft.sum(j1 - 1)\n",
    "        d1 = 0.0 if (j1 == 0) else (tm1 / 2.0)\n",
    "        m1 = s + d1\n",
    "        m2 = m1 + (tm1 - d1) + (tm2 if (j2 == self.nclusters - 1) else (tm2 / 2.0))\n",
    "        m = m1 + (x - c1) * (m2 - m1) * (c2 - c1)\n",
    "        return min(m2, max(m1, m)) / self.mass\n",
    "    \n",
    "    def cdfi(self, q):\n",
    "        if q < 0.0 or q > 1.0 or self.nclusters == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        if self.nclusters == 1:\n",
    "            return self.centers[0]\n",
    "        \n",
    "        m = q * self.mass\n",
    "        j1 = self._rmcovj(m)\n",
    "        j2 = j1 + 1\n",
    "        \n",
    "        c1, c2 = self.centers[j1], self.centers[j2]\n",
    "        tm1, tm2 = self.masses[j1], self.masses[j2]\n",
    "        s = self.ft.sum(j1 - 1)\n",
    "        d1 = 0.0 if (j1 == 0) else (tm1 / 2.0)\n",
    "        m1 = s + d1\n",
    "        m2 = m1 + (tm1 - d1) + (tm2 if (j2 == self.nclusters - 1) else (tm2 / 2.0))\n",
    "        \n",
    "        x = c1 + (m - m1) * (c2 - c1) / (m2 - m1)\n",
    "        return min(c2, max(c1, x))\n",
    "    \n",
    "    def r(self):\n",
    "        return int(TDigest.K / self.compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sketch some data. This data is Poisson distributed. Perhaps it represents query latencies you are measuring for one of your microservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "sketch = TDigest(compression = 0.1)\n",
    "\n",
    "for p in scipy.stats.gamma.rvs(7, 1, size=100000):\n",
    "    sketch.update(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the CDF of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCDF(tdigest):\n",
    "    xmin = sketch.cdfi(0)\n",
    "    xmax = sketch.cdfi(1)\n",
    "    xdata = np.arange(xmin,xmax,0.2).tolist()\n",
    "    ydata = [sketch.cdf(x) for x in xdata]\n",
    "    plt.plot(xdata, ydata)\n",
    "\n",
    "plotCDF(sketch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get quantiles from your data. Here we are looking at median (q=0.5), the 90th percentile and the 99.9th percentile. You can use quantiles such as these to answer questions such as \"am I meeting the desired SLA for query latencies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sketch.cdfi(0.5), sketch.cdfi(0.9), sketch.cdfi(0.999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Transform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(tdigest, size=1):\n",
    "    from scipy.stats import uniform\n",
    "    q = uniform.rvs(size=size).tolist()\n",
    "    return [tdigest.cdfi(y) for y in q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(sketch, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-Digest as a Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchSyn = TDigest(compression = 0.1)\n",
    "for x in sample(sketch, size=10000):\n",
    "    sketchSyn.update(x)\n",
    "\n",
    "plotCDF(sketchSyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
