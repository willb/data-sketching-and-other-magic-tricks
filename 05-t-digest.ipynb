{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating quantiles with the t-digest\n",
    "\n",
    "The [t-digest](https://github.com/tdunning/t-digest) is a compact data structure for summarizing observed cumulative probability distributions.  Like the other structures we've discussed, it's incremental, scalable, and parallel (although the implementation in this notebook is not parallel).  Also like the other structures we've discussed, it has many useful applications in systems, performance analysis, and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from datasketching.fenwicktree import FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will define a `TDigest` class that implements the `update` operation for inserting data elements into the sketch. This tutorial implementation does not provide the `merge` operation, which is useful for combining partial results in a distributed computing setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDigest(object):\n",
    "    COMPRESSION_DEFAULT = 50.0 / 100.0\n",
    "    INIT_SIZE_DEFAULT = 5\n",
    "    K = 10.0 * 50.0\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.centers = np.zeros(1)\n",
    "        self.masses = np.zeros(1)\n",
    "        self.nclusters = 0\n",
    "        self.mass = 0.0\n",
    "    \n",
    "    def __init__(self, compression=COMPRESSION_DEFAULT, max_discrete=0, size=INIT_SIZE_DEFAULT):\n",
    "        self.compression = compression\n",
    "        self.max_discrete = max_discrete\n",
    "        self.ft = FT(size)\n",
    "        self._reset()\n",
    "    \n",
    "    def new_cluster(self, insertion_point, sample, weight):\n",
    "        # note that this will allocate up to two new arrays for \n",
    "        # every created cluster (don't do this in real life!)\n",
    "        \n",
    "        if self.nclusters >= self.ft.size():\n",
    "            self.ft.reset(self.nclusters)\n",
    "        else:\n",
    "            self.ft.reset()\n",
    "        \n",
    "        self.centers = np.insert(self.centers, insertion_point, sample)\n",
    "        self.masses = np.insert(self.masses, insertion_point, weight)\n",
    "        for (k, mass) in enumerate(self.masses):\n",
    "            self.ft.inc_by(k, mass)\n",
    "\n",
    "        self.nclusters += 1\n",
    "        self.mass += weight\n",
    "    \n",
    "    def _closest(self, sample):\n",
    "        insertion_point = np.searchsorted(self.centers, sample, 'right')\n",
    "        \n",
    "        if insertion_point == self.nclusters:\n",
    "            return insertion_point - 1\n",
    "        \n",
    "        if self.centers[insertion_point] == sample \\\n",
    "        or insertion_point == 0:\n",
    "            # the closest cluster center is around this \n",
    "            # sample or is smaller than any cluster center; \n",
    "            # we can rinsertion_pointeturn it unmodified\n",
    "            return insertion_point\n",
    "        \n",
    "        # the insertion point is between two clusters; choose the closest\n",
    "        dl = sample - self.centers[insertion_point - 1]\n",
    "        dr = self.centers[insertion_point] - sample\n",
    "        return (insertion_point - 1) if (dl < dr) else insertion_point\n",
    "            \n",
    "    \n",
    "    def update(self, sample, weight=1.0, maybeRecluster=True):\n",
    "        def _update(sample, weight):\n",
    "            if self.nclusters == 0:\n",
    "                # this is our first observation\n",
    "                self.centers[0] = sample\n",
    "                self.mass = self.masses[0] = weight\n",
    "                self.ft.inc_by(1, weight)\n",
    "                self.nclusters += 1\n",
    "                return\n",
    "                \n",
    "            if self.nclusters <= self.max_discrete:\n",
    "                # we can just introduce a new cluster without\n",
    "                # expanding beyond the maximum number of discrete\n",
    "                # values we're willing to track\n",
    "                insertion_point = np.searchsorted(self.centers, sample, 'right')\n",
    "                if insertion_point < self.nclusters \\\n",
    "                and self.centers[insertion_point] == sample:\n",
    "                    # landed on existing cluster\n",
    "                    self.mass += weight\n",
    "                    self.masses[insertion_point] += weight\n",
    "                    self.ft.inc_by(insertion_point, weight)\n",
    "                else:\n",
    "                    # we need to create a new cluster but don't have\n",
    "                    # to recluster anything\n",
    "                    self.new_cluster(insertion_point, sample, weight)\n",
    "                return\n",
    "\n",
    "            insertion_point = self._closest(sample)\n",
    "            if insertion_point < self.nclusters \\\n",
    "            and sample == self.centers[insertion_point]:\n",
    "                # we have a cluster centered around this sample\n",
    "                self.mass += weight\n",
    "                self.masses[insertion_point] += weight\n",
    "                self.ft.inc_by(insertion_point, weight)\n",
    "                return\n",
    "            \n",
    "            m = self.masses[insertion_point]\n",
    "            \n",
    "            q = (self.ft.sum(insertion_point - 1) + (m / 2.0)) / self.mass\n",
    "            \n",
    "            ub = self.compression * self.mass * q * (1.0 - q)\n",
    "            \n",
    "            delta_mass = min(weight, max(0.0, ub - m))\n",
    "            \n",
    "            remaining_mass = weight - delta_mass\n",
    "            if delta_mass > 0.0:\n",
    "                dc = delta_mass * (sample - self.centers[insertion_point]) / (m + delta_mass)\n",
    "                self.centers[insertion_point] += dc\n",
    "                self.mass += delta_mass\n",
    "                self.masses[insertion_point] += delta_mass\n",
    "                self.ft.inc_by(insertion_point, delta_mass)\n",
    "            if remaining_mass > 0.0:\n",
    "                insertion_point += 0 if (sample < self.centers[insertion_point]) else 1\n",
    "                self.new_cluster(insertion_point, sample, remaining_mass)\n",
    "        \n",
    "        _update(sample, weight)\n",
    "        \n",
    "        if (maybeRecluster \\\n",
    "        and self.nclusters > self.max_discrete \\\n",
    "        and self.nclusters > self.r()):\n",
    "            self.recluster()\n",
    "    \n",
    "    def recluster(self):\n",
    "        indices = list(range(self.nclusters))\n",
    "        np.random.shuffle(indices)\n",
    "        old_centers = self.centers\n",
    "        old_masses = self.masses\n",
    "        self._reset()\n",
    "        \n",
    "        for i in indices:\n",
    "            self.update(old_centers[i], old_masses[i], False)\n",
    "    \n",
    "    def _rcovj(self, x):\n",
    "        \"\"\" return the left index of a right cover \"\"\"\n",
    "        j = np.searchsorted(self.centers, x, 'right')\n",
    "        if j == self.nclusters:\n",
    "            return j - 1\n",
    "        \n",
    "        if self.centers[j] == x:\n",
    "            return j\n",
    "        \n",
    "        return j - 1\n",
    "    \n",
    "    def _rmcovj(self, m):\n",
    "        assert(self.nclusters >= 2)\n",
    "        assert((m >= 0.0) and (m <= self.mass))\n",
    "        beg = 0\n",
    "        mbeg = 0.0\n",
    "        end = self.nclusters - 1\n",
    "        mend = self.mass\n",
    "        while (end - beg) > 1:\n",
    "            mid = int((beg + end) / 2)\n",
    "            mmid = self.ft.sum(mid)\n",
    "            if m >= mmid:\n",
    "                beg = mid\n",
    "                mbeg = mmid\n",
    "            else:\n",
    "                end = mid\n",
    "                mend = mmid\n",
    "        return beg\n",
    "        \n",
    "    def cdf(self, x):\n",
    "        j1 = self._rcovj(x)\n",
    "        if j1 < 0:\n",
    "            return 0.0\n",
    "        if j1 == self.nclusters - 1:\n",
    "            return 1.0\n",
    "        \n",
    "        j2 = j1 + 1\n",
    "        c1, c2 = self.centers[j1], self.centers[j2]\n",
    "        tm1, tm2 = self.masses[j1], self.masses[j2]\n",
    "        s = self.ft.sum(j1 - 1)\n",
    "        d1 = 0.0 if (j1 == 0) else (tm1 / 2.0)\n",
    "        m1 = s + d1\n",
    "        m2 = m1 + (tm1 - d1) + (tm2 if (j2 == self.nclusters - 1) else (tm2 / 2.0))\n",
    "        m = m1 + (x - c1) * (m2 - m1) * (c2 - c1)\n",
    "        return min(m2, max(m1, m)) / self.mass\n",
    "    \n",
    "    def cdfi(self, q):\n",
    "        if q < 0.0 or q > 1.0 or self.nclusters == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        if self.nclusters == 1:\n",
    "            return self.centers[0]\n",
    "        \n",
    "        m = q * self.mass\n",
    "        j1 = self._rmcovj(m)\n",
    "        j2 = j1 + 1\n",
    "        \n",
    "        c1, c2 = self.centers[j1], self.centers[j2]\n",
    "        tm1, tm2 = self.masses[j1], self.masses[j2]\n",
    "        s = self.ft.sum(j1 - 1)\n",
    "        d1 = 0.0 if (j1 == 0) else (tm1 / 2.0)\n",
    "        m1 = s + d1\n",
    "        m2 = m1 + (tm1 - d1) + (tm2 if (j2 == self.nclusters - 1) else (tm2 / 2.0))\n",
    "        \n",
    "        x = c1 + (m - m1) * (c2 - c1) / (m2 - m1)\n",
    "        return min(c2, max(c1, x))\n",
    "    \n",
    "    def r(self):\n",
    "        return int(TDigest.K / self.compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketching data with the t-digest\n",
    "\n",
    "This data has a similar shape to the Poisson distribution, which means that it could resemble latencies for a network service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "sketch = TDigest(compression = 0.1)\n",
    "\n",
    "for p in scipy.stats.gamma.rvs(7, 1, size=100000):\n",
    "    sketch.update(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCDF(tds):\n",
    "    \"\"\" plots the CDF of the supplied t-digest sketch \"\"\"\n",
    "    import pandas as pd\n",
    "    import altair as alt\n",
    "    alt.renderers.enable(\"notebook\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"x\"] = np.arange(tds.cdfi(0), tds.cdfi(1)).tolist()\n",
    "    df[\"y\"] = [tds.cdf(x) for x in df[\"x\"]]\n",
    "\n",
    "    return alt.Chart(df).mark_line().encode(x=\"x\", y=\"y\")\n",
    "\n",
    "plotCDF(sketch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get quantiles from your data. \n",
    "\n",
    "Here we are looking at median (q=0.5), the 90th percentile and the 99.9th percentile. You can use quantiles such as these to answer questions such as \"am I meeting the desired SLA for query latencies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sketch.cdfi(0.5), sketch.cdfi(0.9), sketch.cdfi(0.999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Transform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(tdigest, size=1):\n",
    "    from scipy.stats import uniform\n",
    "    q = uniform.rvs(size=size).tolist()\n",
    "    return [tdigest.cdfi(y) for y in q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(sketch, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the t-digest as a generative model\n",
    "\n",
    "A very cool aspect of the t-digest is that you can use it to observe data and then generate simulated data with a similar shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchSyn = TDigest(compression = 0.1)\n",
    "for x in sample(sketch, size=10000):\n",
    "    sketchSyn.update(x)\n",
    "\n",
    "plotCDF(sketchSyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "* Try sketching streams sampled from some other random sources (use `dir(scipy.stats)` to see probability distributions available to you in `scipy`) and sampling from their t-digests.\n",
    "* A great application for t-digests is in adaptive outlier detection.  Consider how you'd use a t-digest to identify outliers or anomalies in a stream of metrics.\n",
    "* Read [this blog post](http://erikerlandson.github.io/blog/2016/12/19/converging-monoid-addition-for-t-digest/) and then implement a merge operation for the t-digest implementation in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
