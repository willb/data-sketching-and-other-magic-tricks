{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minhash for recommendations\n",
    "\n",
    "In the [previous notebook](04-minhash.ipynb) we saw how Minhash can be used to approximate the similarity of sets. In this notebook we will see that Minhash can also be used to make recommendations. \n",
    "\n",
    "We illustrate this technique using a data set which contains users' listening history from a music streaming service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19098853, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/music.parquet\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains three columns and over 19million rows. Let's take a closer look at a sample of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12708245</th>\n",
       "      <td>user_000684</td>\n",
       "      <td>2006-11-20T00:52:10Z</td>\n",
       "      <td>The Tragically Hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574545</th>\n",
       "      <td>user_000023</td>\n",
       "      <td>2007-11-17T09:00:20Z</td>\n",
       "      <td>Rebekah Del Rio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11024724</th>\n",
       "      <td>user_000590</td>\n",
       "      <td>2008-06-15T20:16:28Z</td>\n",
       "      <td>Explosions In The Sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138851</th>\n",
       "      <td>user_000366</td>\n",
       "      <td>2006-11-18T00:59:37Z</td>\n",
       "      <td>Meltiis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982242</th>\n",
       "      <td>user_000427</td>\n",
       "      <td>2007-12-23T06:33:29Z</td>\n",
       "      <td>Derrick Morgan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7447167</th>\n",
       "      <td>user_000384</td>\n",
       "      <td>2008-07-04T14:30:41Z</td>\n",
       "      <td>Girl Talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17460811</th>\n",
       "      <td>user_000906</td>\n",
       "      <td>2006-09-18T12:25:52Z</td>\n",
       "      <td>Creedence Clearwater Revisited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15220317</th>\n",
       "      <td>user_000793</td>\n",
       "      <td>2007-09-27T17:38:17Z</td>\n",
       "      <td>Lightning Bolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13575207</th>\n",
       "      <td>user_000714</td>\n",
       "      <td>2009-03-16T06:26:37Z</td>\n",
       "      <td>Britney Spears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938512</th>\n",
       "      <td>user_000147</td>\n",
       "      <td>2006-05-08T19:43:47Z</td>\n",
       "      <td>Broadcast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                     1                               2\n",
       "12708245  user_000684  2006-11-20T00:52:10Z              The Tragically Hip\n",
       "574545    user_000023  2007-11-17T09:00:20Z                 Rebekah Del Rio\n",
       "11024724  user_000590  2008-06-15T20:16:28Z           Explosions In The Sky\n",
       "7138851   user_000366  2006-11-18T00:59:37Z                         Meltiis\n",
       "7982242   user_000427  2007-12-23T06:33:29Z                  Derrick Morgan\n",
       "7447167   user_000384  2008-07-04T14:30:41Z                       Girl Talk\n",
       "17460811  user_000906  2006-09-18T12:25:52Z  Creedence Clearwater Revisited\n",
       "15220317  user_000793  2007-09-27T17:38:17Z                  Lightning Bolt\n",
       "13575207  user_000714  2009-03-16T06:26:37Z                  Britney Spears\n",
       "2938512   user_000147  2006-05-08T19:43:47Z                       Broadcast"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains three columns - the first ['0'] is a user id, the second ['1'] is a timestamp representing when the user listened to the song and the artist is named in column ['2'].\n",
    "\n",
    "We take one pass through the data to identify all unique artists all unique users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = df['2'].unique()\n",
    "users = df['0'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map those artist names and user names to unique integers and store those in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dartists = {y:x+1 for x, y in enumerate(sorted(set(artists)))}\n",
    "dartists2 = {x+1:y for x,y in enumerate(sorted(set(artists)))}\n",
    "dusers = {y:x+1 for x,y in enumerate(sorted(set(users)))}\n",
    "dusers2 = {x+1:y for x,y in enumerate(sorted(set(users)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 173921 artists and 992 users in our data.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \", len(dartists), \" artists and \", len(dusers), \" users in our data.\" , sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the data set by user. From there we can see which artists a particular user has listened to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_data(user, grouped_data, dusers2):\n",
    "    return grouped_data.get_group(dusers2[user]) \n",
    "\n",
    "def top_k_listens(listening_history, k=10):\n",
    "    hist = listening_history.groupby(['2'])\n",
    "    return hist.count().sort_values(by='0', ascending=False).head(k).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = user_data(2, grouped_df, dusers2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Libertines', 'Babyshambles', 'Kettcar', 'The Kooks',\n",
       "       'Maxïmo Park', 'Death Cab For Cutie', 'Sophie Milman',\n",
       "       'Bright Eyes', 'Adam Green', 'Peter Doherty'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(top_k_listens(u2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user, we want to generate a minhash of their listening history. The minhash class which we used in the previous notebook has been put into its own module for ease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketching.minhash import SimpleMinhash\n",
    "from datasketching.minhash import murmurmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minhash_sig(user_dat, nhash):\n",
    "    mh = SimpleMinhash(nhash)\n",
    "    for row in user_dat:\n",
    "        mh.add(row)\n",
    "    return mh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for each user, we want to compose a list of all the artists they listened to. From there we will generate minhashes for each user, then make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_artists(df):\n",
    "    return df['2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_artists = grouped_df.apply(unique_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_sigs = un_artists.apply(generate_minhash_sig, nhash = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have minhash signatures for all of the users we can compare them. But this isnt a quick process - suppose we want to find users who are similar to user 2. \n",
    "\n",
    "In order to do that we have to compare 997 pairs of users. This isn't too bad, but clearly is going to get out of hand as the number of users we want to make recomendations for, and the number of users in the data set grows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code which runs the comparison for user 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim=[]\n",
    "for mh in range(0, 992):\n",
    "    sim.append(mh_sigs[mh].similarity(mh_sigs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the users who are most similar to user 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1796875, 0.1875, 0.1953125, 0.203125, 0.2265625}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = set(sorted(sim, reverse = True)[1:10])\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_users = ([i for i, e in enumerate(sim) if e in similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most similar users. Let's go ahead and look at the top artists listened to by all these users. \n",
    "\n",
    "Going to look at the unique artists listened to by each of these, remove uniques listened to by user 2, and then return the most listened across the other users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the ?top 10 artists most listened to by these users that our user didnt listen to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_data(user, grouped_data, dusers2):\n",
    "    return grouped_data.get_group(dusers2[user]) \n",
    "\n",
    "def top_k_listens(listening_history, k=10):\n",
    "    hist = listening_history.groupby(['2'])\n",
    "    return hist.count().sort_values(by='0', ascending=False).head(k).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Libertines', 'Babyshambles', 'Kettcar', 'The Kooks',\n",
       "       'Maxïmo Park', 'Death Cab For Cutie', 'Sophie Milman',\n",
       "       'Bright Eyes', 'Adam Green', 'Peter Doherty'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2 = user_data(2, grouped_df, dusers2)\n",
    "top_k_listens(u2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unheard = []\n",
    "for u in similar_users:\n",
    "    u_dat = user_data(u, grouped_df, dusers2)\n",
    "    unheard = unheard + list(top_k_listens(u_dat, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2Pac', 'A Boy Named Thor', 'Afi', 'Akira Yamaoka', 'Amaral',\n",
       "       'Annihilator', 'Baba Zula', 'Black Sabbath', 'Brigitte Bardot',\n",
       "       'Bt', 'Camel', 'Chris Vrenna', 'Depeche Mode', 'Franz Schubert',\n",
       "       'Frédéric Chopin', 'Genesis', 'Giuseppe Verdi', 'Gogol Bordello',\n",
       "       'Gustavo Santaolalla', 'Göksel Baktagir', 'Infusion',\n",
       "       'Iron Maiden', 'Jane Birkin & Serge Gainsbourg', 'Johannes Brahms',\n",
       "       'Jurassic 5', 'Laura Pausini', 'Luz Casal', 'M. Ward', 'Mae',\n",
       "       'Megadeth', 'Mercan Dede', 'Mew', 'Ministry Of Sound', 'Mis-Teeq',\n",
       "       'N.E.R.D.', 'Orbital', 'Ozric Tentacles', 'Paul Mccartney',\n",
       "       'Queens Of The Stone Age', 'Rush', 'Scorpions', 'Serge Gainsbourg',\n",
       "       'Serj Tankian', 'Story One', 'The Bear Quartet',\n",
       "       'The Crystal Method', 'The Minders', 'The Twin Atlas', 'Tool',\n",
       "       'Tori Amos', 'Unkle', 'Сергей Васильевич Рахманинов',\n",
       "       'Сергей Сергеевич Прокофьев'], dtype='<U30')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(unheard, un_artists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!!!', 1),\n",
       " ('+/-', 1),\n",
       " ('1 Mile North', 1),\n",
       " ('10,000 Maniacs', 1),\n",
       " ('13 & God', 1),\n",
       " ('1800S Sea Monster', 1),\n",
       " ('1990S', 1),\n",
       " (\"2 Many Dj'S\", 1),\n",
       " ('2Pac', 1),\n",
       " ('4Hero', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also tried an alternative method, lookin at \"most listened to by all those similar users\",\n",
    "# but once you remove everything user 2 listens to, there is no artist listened to by multiple users! \n",
    "\n",
    "import numpy as np\n",
    "unheard = []\n",
    "for u in similar_users:\n",
    "    unheard = unheard + list(np.setdiff1d(un_artists[u], un_artists[1]))\n",
    "    \n",
    "from itertools import groupby\n",
    "freqs = [(key, len(list(group))) for key, group in groupby(unheard)]\n",
    "\n",
    "sorted_fr = sorted(freqs, key = lambda i: i[1],reverse=False) \n",
    "sorted_fr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Minhash\n",
    "\n",
    "One big disadvantage of using Minhash signatures to identify similar users is the number of pairwise comparisons which must be made to determine similarity. \n",
    "\n",
    "Locality-sensitive Minhash is a technique we can use to identify candidate pairs of similar users for a much smaller computational cost. The method works by hashing subsets minhash signatures. If 2 users have identical signatures in ANY of the subsets these users are considered a candidate pair. And from there you can go and compute their approximate Jaccard index, or similarity, using the full minhash signatures, to determine just how similar they are, and decide if you want to make recommendations. \n",
    "\n",
    "The way in locality sensitive minhash works is by splitting the minhash signatures into bands. The bands are then hashed to buckets. \n",
    "\n",
    "if, in any band, two users map to the same bucket, they would be considered a candidate pair. At that point you’d go back and look at their minhash signatures, and compare those to determine how similar the users are. \n",
    "\n",
    "\n",
    "And thus we only have to compute the similarity of the minhash signatures for a subset of the whole population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
